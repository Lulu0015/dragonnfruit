{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import numpy\n",
    "import scipy\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from dragonnfruit.io import LocusGenerator\n",
    "from dragonnfruit.io import GenomewideGenerator\n",
    "\n",
    "from dragonnfruit.models import CellStateController\n",
    "from dragonnfruit.models import DynamicBPNet\n",
    "from dragonnfruit.models import DragoNNFruit\n",
    "from dragonnfruit.io import load_data, save_data\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonnfruit.preprocessing import read_chrom_sizes\n",
    "\n",
    "chrom_sizes, header = read_chrom_sizes(\"/workspaces/torch_ddsm/_data_pool1/general/hg38.chrom.sizes\")\n",
    "canonical_chroms = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', \n",
    "\t\t'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', \n",
    "\t\t'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', \n",
    "\t\t'chr22', 'chrX']\n",
    "filtered_chrom_sizes = {key: value for key, value in chrom_sizes.items() if key in canonical_chroms}\n",
    "chroms_prepro = list(filtered_chrom_sizes.keys())\n",
    "\n",
    "test_chroms = ['chr2']\n",
    "validation_chroms = ['chr8', 'chr10']\n",
    "\n",
    "training_chroms = [chrom for chrom in chroms_prepro if chrom not in test_chroms + validation_chroms]\n",
    "chroms = training_chroms + validation_chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 256\n",
    "n_layers = 8\n",
    "\n",
    "batch_size = 256\n",
    "window = 2114\n",
    "trimming = (window - 1000) // 2\n",
    "\n",
    "data_dir = \"/workspaces/torch_ddsm/_data_pool1\"\n",
    "peak_file = f\"{data_dir}/10x_data/pbmc3kv2/pbmc_granulocyte_sorted_3k_atac_peaks.bed\"\n",
    "run_prefix = \"fit_atac_pca\"\n",
    "os.makedirs(f\"{data_dir}/test_dragonnfruit_pbmc/{run_prefix}\", exist_ok=True)\n",
    "\n",
    "###\n",
    "# Load single-cell data\n",
    "#print(\"Loading sc-data\")\n",
    "\n",
    "k = 500\n",
    "n_nodes = 1024\n",
    "n_outputs = 256\n",
    "\n",
    "neighbors = numpy.load(f\"{data_dir}/test_dragonnfruit_pbmc/atac_neighbors.npz\")['arr_0'][:, :k]\n",
    "cell_states = numpy.load(f\"{data_dir}/test_dragonnfruit_pbmc/atac_pca.npz\")['arr_0'].astype('float32')\n",
    "cell_states = (cell_states - cell_states.mean(axis=0, keepdims=True)) / cell_states.std(axis=0, keepdims=True)\n",
    "\n",
    "read_depths = numpy.load(f\"{data_dir}/test_dragonnfruit_pbmc/atac_read_depths.npz\")['arr_0'].astype('float32')\n",
    "read_depths = read_depths[neighbors].sum(axis=1)\n",
    "read_depths = numpy.log2(read_depths + 1)\n",
    "read_depths = read_depths.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the single cell atac signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cscs = load_data(f\"{data_dir}/test_dragonnfruit_pbmc/dragonnfruit_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enumerate the one-hot sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding sequence chr12\n",
      "dim: (4, 133275309)\n",
      "encoding sequence chr15\n",
      "dim: (4, 101991189)\n",
      "encoding sequence chr4\n",
      "dim: (4, 190214555)\n",
      "encoding sequence chr22\n",
      "dim: (4, 50818468)\n",
      "encoding sequence chr5\n",
      "dim: (4, 181538259)\n",
      "encoding sequence chr19\n",
      "dim: (4, 58617616)\n",
      "encoding sequence chr8\n",
      "dim: (4, 145138636)\n",
      "encoding sequence chr11\n",
      "dim: (4, 135086622)\n",
      "encoding sequence chr1\n",
      "dim: (4, 248956422)\n",
      "encoding sequence chr21\n",
      "dim: (4, 46709983)\n",
      "encoding sequence chr7\n",
      "dim: (4, 159345973)\n",
      "encoding sequence chr20\n",
      "dim: (4, 64444167)\n",
      "encoding sequence chr13\n",
      "dim: (4, 114364328)\n",
      "encoding sequence chr9\n",
      "dim: (4, 138394717)\n",
      "encoding sequence chr2\n",
      "dim: (4, 242193529)\n",
      "encoding sequence chr17\n",
      "dim: (4, 83257441)\n",
      "encoding sequence chrX\n",
      "dim: (4, 156040895)\n",
      "encoding sequence chr6\n",
      "dim: (4, 170805979)\n",
      "encoding sequence chr18\n",
      "dim: (4, 80373285)\n",
      "encoding sequence chr10\n",
      "dim: (4, 133797422)\n",
      "encoding sequence chr16\n",
      "dim: (4, 90338345)\n",
      "encoding sequence chr14\n",
      "dim: (4, 107043718)\n",
      "encoding sequence chr3\n",
      "dim: (4, 198295559)\n",
      "adding chr12 to hdf5\n",
      "adding chr15 to hdf5\n",
      "adding chr4 to hdf5\n",
      "adding chr22 to hdf5\n",
      "adding chr5 to hdf5\n",
      "adding chr19 to hdf5\n",
      "adding chr8 to hdf5\n",
      "adding chr11 to hdf5\n",
      "adding chr1 to hdf5\n",
      "adding chr21 to hdf5\n",
      "adding chr7 to hdf5\n",
      "adding chr20 to hdf5\n",
      "adding chr13 to hdf5\n",
      "adding chr9 to hdf5\n",
      "adding chr2 to hdf5\n",
      "adding chr17 to hdf5\n",
      "adding chrX to hdf5\n",
      "adding chr6 to hdf5\n",
      "adding chr18 to hdf5\n",
      "adding chr10 to hdf5\n",
      "adding chr16 to hdf5\n",
      "adding chr14 to hdf5\n",
      "adding chr3 to hdf5\n"
     ]
    }
   ],
   "source": [
    "import pyfaidx\n",
    "from bpnetlite.io import one_hot_encode\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "\n",
    "sequences_faidx = pyfaidx.Fasta('/workspaces/torch_ddsm/_data_pool1/10x_data/refdata-gex-GRCh38-2020-A/fasta/genome.fa')\n",
    "\n",
    "signals = {}\n",
    "sequences = {}\n",
    "for key in X_cscs.keys():\n",
    "    print(f\"encoding sequence {key}\")\n",
    "    sequences[key] = one_hot_encode(sequences_faidx[key][:].seq.upper(),\n",
    "                                    alphabet=['A', 'C', 'G', 'T'])\n",
    "    print(f\"dim: {sequences[key].shape}\")\n",
    "    signals[key] = X_cscs[key]\n",
    "\n",
    "outfile = h5py.File(f\"{data_dir}/test_dragonnfruit_pbmc/{run_prefix}/chrom_onehots.h5\", 'w')\n",
    "for chrom, data in sequences.items():\n",
    "    print(f\"adding {chrom} to hdf5\")\n",
    "    outfile.create_dataset(chrom, data=data.data, **hdf5plugin.Blosc(clevel=9))\n",
    "# Close the HDF5 file\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the sequences from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Define the path to the HDF5 file\n",
    "file_path = f\"{data_dir}/test_dragonnfruit_pbmc/{run_prefix}/chrom_onehots.h5\"\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(file_path, 'r') as infile:\n",
    "    sequences = {}\n",
    "    \n",
    "    # Iterate through the datasets in the HDF5 file\n",
    "    for chrom in infile.keys():\n",
    "        # Read the data for each dataset (chromosome)\n",
    "        data = infile[chrom][()]\n",
    "        \n",
    "        # Store the data in the sequences dictionary\n",
    "        sequences[chrom] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tIteration\tTraining Time\tValidation Time\tTraining MNLL\tValidation MNLL\tValidation Profile Correlation\tValidation Count Correlation\tSaved?\n",
      "0\t0\t5.1913\t0.7624\t25.8086\t0.0\t0.0\t0.0\tFalse\n",
      "0\t1000\t142.7383\t0.0934\t17.8639\t0.0\t0.0\t0.0\tFalse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m DragoNNFruit(bias_model, accessibility_model, name)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     39\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_validation_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43mvalidation_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# model.fit(X, X_valid, optimizer, n_validation_samples=50324, max_epochs=50, \u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# validation_iter=1000, batch_size=batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/torch_ddsm/dragonnfruit/dragonnfruit/models.py:326\u001b[0m, in \u001b[0;36mDragoNNFruit.fit\u001b[0;34m(self, training_data, validation_data, optimizer, n_validation_samples, max_epochs, batch_size, validation_iter, verbose)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m    325\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m i, (X, y, cell_states, read_depths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_data):\n\u001b[0;32m--> 326\u001b[0m \t\tX \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    327\u001b[0m \t\ty \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    328\u001b[0m \t\tcell_states \u001b[38;5;241m=\u001b[39m cell_states\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(\"Done loading sc-data\")\n",
    "###\n",
    "\n",
    "X = torch.utils.data.DataLoader(\n",
    "    GenomewideGenerator(\n",
    "        sequence=sequences,\n",
    "        signal=signals,\n",
    "        neighbors=neighbors,\n",
    "        cell_states=cell_states,\n",
    "        read_depths=read_depths,\n",
    "        trimming=trimming, \n",
    "        window=window, \n",
    "        chroms=training_chroms,\n",
    "        random_state=None),\n",
    "        pin_memory=True, \n",
    "        num_workers=8,\n",
    "        worker_init_fn=lambda x: numpy.random.seed(x),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "X_valid = LocusGenerator(\n",
    "    sequence=sequences,\n",
    "    signal=signals,\n",
    "    loci_file=peak_file,\n",
    "    neighbors=neighbors,\n",
    "    cell_states=cell_states,\n",
    "    read_depths=read_depths,\n",
    "    trimming=trimming, \n",
    "    window=window,\n",
    "    chroms=validation_chroms,\n",
    "    random_state=0)\n",
    "\n",
    "bias_model = torch.load(\"/workspaces/torch_ddsm/_data_pool1/test_dragonnfruit_pbmc/pbmc_granulocyte_sorted_3k_atac.final.torch\", map_location='cpu').cuda()\n",
    "controller = CellStateController(n_inputs=cell_states.shape[-1], n_nodes=n_nodes, n_layers=1, n_outputs=n_outputs).cuda()\n",
    "accessibility_model = DynamicBPNet(n_filters=n_filters, n_layers=n_layers, trimming=trimming, controller=controller).cuda()\n",
    "\n",
    "name = \"{}/test_dragonnfruit_pbmc/{}/dragonnfruit.fibr.{}.{}.{}.{}.{}\".format(data_dir,run_prefix,n_filters, n_layers, k, n_nodes, n_outputs)\n",
    "model = DragoNNFruit(bias_model, accessibility_model, name).cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "model.fit(X, X_valid, optimizer, n_validation_samples=50, max_epochs=5, \n",
    "validation_iter=1000, batch_size=batch_size)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "# model.fit(X, X_valid, optimizer, n_validation_samples=50324, max_epochs=50, \n",
    "# validation_iter=1000, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
